{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENWWWXQdHtvz"
      },
      "outputs": [],
      "source": [
        "!pip install pymc==5.10.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import datetime"
      ],
      "metadata": {
        "id": "pJTdol9rXcqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update with the appropriate training data path\n",
        "df_winter = pd.read_csv('',index_col=0, parse_dates=True)\n",
        "\n",
        "df_winter.head()"
      ],
      "metadata": {
        "id": "Ng_cg8uuXfpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter.isnull().sum()"
      ],
      "metadata": {
        "id": "TV1C20MpYT-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter = df_winter.dropna()"
      ],
      "metadata": {
        "id": "Ph7zrsImYYMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter.isnull().sum()"
      ],
      "metadata": {
        "id": "o_Y7Lsq9YbeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter['fog_index_1d'].eq(0).sum()"
      ],
      "metadata": {
        "id": "0hP5CdXPYhEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter.shape"
      ],
      "metadata": {
        "id": "ICNCQ1C_Ymvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_winter[df_winter['fog_index_1d'] != 0]"
      ],
      "metadata": {
        "id": "Igc3ZrsyYq61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter = df_filtered"
      ],
      "metadata": {
        "id": "nsrBcNQFY6Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter.shape"
      ],
      "metadata": {
        "id": "egUnM5UkY86a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter['fog_index_1d'].eq(0).sum()"
      ],
      "metadata": {
        "id": "bv9ODP3_Y_s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter = df_winter[df_winter['energy_loss'] <= 1500000]"
      ],
      "metadata": {
        "id": "h_ShPTgwZEuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter.shape"
      ],
      "metadata": {
        "id": "SMBR5H9oZRfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "wiAZl0zhZUvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_winter.drop([\"fog_index_6h\",\"fog_index_1d\"], axis=1)  # Features\n",
        "y = df_winter[\"fog_index_1d\"] # Target variable"
      ],
      "metadata": {
        "id": "wHEt8iDuailm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X)  # Fit scaler on training data\n",
        "\n",
        "X_train_scaled = scaler.transform(X)\n"
      ],
      "metadata": {
        "id": "Mb-qrJAqaz5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)"
      ],
      "metadata": {
        "id": "ZpCcKxoQa4OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled_df.columns"
      ],
      "metadata": {
        "id": "I3vbzlpOa-vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y"
      ],
      "metadata": {
        "id": "cO7pgYsqTMHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "\n",
        "with pm.Model() as beta_regression_model:\n",
        "    # Priors\n",
        "    beta_0 = pm.Normal('beta_0', mu=0, sigma=100)\n",
        "    beta_1 = pm.Normal('beta_1', mu=0, sigma=100)\n",
        "    beta_2 = pm.Normal('beta_2', mu=0, sigma=100)\n",
        "    beta_3 = pm.Normal('beta_3', mu=0, sigma=100)\n",
        "    beta_4 = pm.Normal('beta_4', mu=0, sigma=100)\n",
        "    beta_5 = pm.Normal('beta_5', mu=0, sigma=100)\n",
        "    beta_6 = pm.Normal('beta_6', mu=0, sigma=100)\n",
        "    beta_7 = pm.Normal('beta_7', mu=0, sigma=100)\n",
        "    beta_8 = pm.Normal('beta_8', mu=0, sigma=100)\n",
        "\n",
        "    avg_air_temp = pm.MutableData('avg_air_temp', X_train_scaled_df['avg_air_temp'])\n",
        "    avg_dew_point = pm.MutableData('avg_dew_point', X_train_scaled_df['avg_dew_point'])\n",
        "    avg_relative_humidity = pm.MutableData('avg_relative_humidity', X_train_scaled_df['avg_relative_humidity'])\n",
        "    avg_pressure = pm.MutableData('avg_pressure', X_train_scaled_df['avg_pressure'])\n",
        "    avg_visibility = pm.MutableData('avg_visibility', X_train_scaled_df['avg_visibility'])\n",
        "    energy_loss = pm.MutableData('energy_loss', X_train_scaled_df['energy_loss'])\n",
        "    fog_duration = pm.MutableData('fog_duration', X_train_scaled_df['fog_duration'])\n",
        "    fog_month = pm.MutableData('fog_month', X_train_scaled_df['fog_month'])\n",
        "\n",
        "    fog_index_1d = pm.MutableData('fog_index', y_train)\n",
        "\n",
        "    alpha = pm.Uniform('alpha', lower=0, upper=200)\n",
        "    beta = pm.Uniform('beta', lower=0, upper=200)\n",
        "\n",
        "    # Precision parameterization\n",
        "    phi = alpha + beta\n",
        "\n",
        "\n",
        "    # Mean of the beta distribution\n",
        "    mu = pm.Deterministic('mu', pm.math.invlogit( beta_0 + beta_1 * avg_air_temp +\n",
        "                          beta_2 * avg_dew_point +\n",
        "                          beta_3 * avg_relative_humidity +\n",
        "                          beta_4 * avg_pressure +\n",
        "                          beta_5 * avg_visibility +\n",
        "                          beta_6 * energy_loss +\n",
        "                          beta_7 * fog_duration +\n",
        "                          beta_8 * fog_month\n",
        "                          ))\n",
        "\n",
        "    # Likelihood\n",
        "    fog_index_var = pm.Beta('fog_index_var', alpha=mu * phi, beta=(1 - mu) * phi, observed=fog_index_1d)\n",
        "\n",
        "    # Inference\n",
        "    trace = pm.sample(draws=2000, tune=2000, chains=2,return_inferencedata=True)"
      ],
      "metadata": {
        "id": "oYdRgVCzbJvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arviz as az\n",
        "az.plot_trace(trace)"
      ],
      "metadata": {
        "id": "-n7DoDcufp_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arviz as az\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "MMRMicIZft8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with beta_regression_model:  # Reuse the model context\n",
        "    ppd = pm.sample_posterior_predictive(\n",
        "        trace,\n",
        "        var_names=['mu','fog_index_var'],  # Name of our output variable\n",
        "        random_seed=42  # Optional\n",
        "    )\n",
        "    az.plot_ppc(ppd)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OUw9SDNVfvTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trace"
      ],
      "metadata": {
        "id": "T3maTXTwgz4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppd"
      ],
      "metadata": {
        "id": "bPva0XpBg1DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update with the appropriate validation data path\n",
        "\n",
        "df_winter_val = pd.read_csv('',index_col=0, parse_dates=True)\n",
        "\n",
        "df_winter_val = df_winter_val.dropna()\n",
        "\n",
        "X_val = df_winter_val.drop([\"fog_index_6h\",\"fog_index_1d\"], axis=1)  # Features\n",
        "y_val = df_winter_val[\"fog_index_1d\"]  # Target variable\n",
        "\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)"
      ],
      "metadata": {
        "id": "Yog74h71TXXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with beta_regression_model:\n",
        "    pm.set_data({\n",
        "        'avg_air_temp': X_val_scaled_df['avg_air_temp'],\n",
        "        'avg_dew_point': X_val_scaled_df['avg_dew_point'],\n",
        "        'avg_relative_humidity': X_val_scaled_df['avg_relative_humidity'],\n",
        "        'avg_pressure': X_val_scaled_df['avg_pressure'],\n",
        "        'avg_visibility': X_val_scaled_df['avg_visibility'],\n",
        "        'energy_loss': X_val_scaled_df['energy_loss'],\n",
        "        'fog_duration': X_val_scaled_df['fog_duration'],\n",
        "        'fog_month': X_val_scaled_df['fog_month'],\n",
        "        'fog_index': y_val\n",
        "    })\n",
        "\n",
        "    idata_val = pm.sample_posterior_predictive(\n",
        "                trace,\n",
        "                var_names=[\"mu\",\"fog_index_var\"],\n",
        "                return_inferencedata=True,\n",
        "                predictions=True,\n",
        "                extend_inferencedata=True,\n",
        "                random_seed=42  # Or any seed\n",
        "     )"
      ],
      "metadata": {
        "id": "tVT5yZVTg4jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idata_val"
      ],
      "metadata": {
        "id": "iq2WpRN1hJRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(idata_val)"
      ],
      "metadata": {
        "id": "dped6SMMhNMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(idata_val.predictions)"
      ],
      "metadata": {
        "id": "RohcwwcvhRYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_for_val_point = idata_val.predictions['fog_index_var']\n",
        "\n",
        "# HDI Calculation\n",
        "hdi = pm.hdi(predictions_for_val_point, hdi_prob=0.95)\n",
        "print(hdi)"
      ],
      "metadata": {
        "id": "sfo4zvCyhVW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_point_predictions_mean = idata_val.predictions['fog_index_var'].mean(dim=['chain', 'draw']).values\n",
        "val_point_predictions_median = idata_val.predictions['fog_index_var'].median(dim=['chain', 'draw']).values"
      ],
      "metadata": {
        "id": "AK5ojICRRQ4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_intervals(hdi,y_test,pp_mean,pp_median):\n",
        "  test_lower_bounds = []\n",
        "  test_upper_bounds = []\n",
        "  test_original_values = []\n",
        "  test_point_pred_mean = []\n",
        "  test_point_pred_median =[]\n",
        "  test_lower_upper_avg = []\n",
        "  test_dates = []\n",
        "\n",
        "  for idx in range(len(hdi['fog_index_var_dim_2'])):\n",
        "    lower_bound = hdi.sel(fog_index_var_dim_2=idx, hdi='lower')['fog_index_var'].values\n",
        "    upper_bound = hdi.sel(fog_index_var_dim_2=idx, hdi='higher')['fog_index_var'].values\n",
        "    avg = (upper_bound - lower_bound)/2\n",
        "\n",
        "    # print(lower_bound['fog_index_var'].values)\n",
        "\n",
        "    point_pred_mean = pp_mean[idx]\n",
        "    point_pred_median = pp_median[idx]\n",
        "    # lb = lower_bound['fog_index_var'].values\n",
        "    # ub = upper_bound['fog_index_var'].values\n",
        "    # break\n",
        "    original = y_test[idx]\n",
        "    date = y_test.index[idx]\n",
        "    # print(f\"Test Point: {idx}, Range: ({lower_bound:.2f}, {upper_bound:.2f}) ,point mean: {point_pred_mean:.2f},point median: {point_pred_median:.2f}, Original: {original:.2f}, Date: {date}\")\n",
        "    test_lower_bounds.append(lower_bound)\n",
        "    test_upper_bounds.append(upper_bound)\n",
        "    test_point_pred_mean.append(point_pred_mean)\n",
        "    test_point_pred_median.append(point_pred_median)\n",
        "    test_lower_upper_avg.append(avg)\n",
        "    test_original_values.append(original)\n",
        "    test_dates.append(date)\n",
        "\n",
        "  data = {\n",
        "    'lower_bound': test_lower_bounds,\n",
        "    'upper_bound': test_upper_bounds,\n",
        "    'original': test_original_values,\n",
        "    'point_pred_mean': test_point_pred_mean,\n",
        "    'point_pred_median': test_point_pred_median,\n",
        "    'lower_upper_avg': test_lower_upper_avg,\n",
        "    'date': test_dates\n",
        "  }\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "SWbXHoP1rMxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def evaluate_range_predictions(data):\n",
        "    df = pd.DataFrame(data)  # Ensure DataFrame format\n",
        "\n",
        "    # Coverage: Proportion of original values within bounds\n",
        "    coverage = (df['original'] >= df['lower_bound']) & (df['original'] <= df['upper_bound'])\n",
        "    coverage_pct = coverage.mean() * 100\n",
        "\n",
        "    # Average range width\n",
        "    average_range_width = df['upper_bound'] - df['lower_bound']\n",
        "    average_range_width = average_range_width.mean()\n",
        "\n",
        "    filtered_df = df[df['original'] > 0.5]  # Filter for points > 0.5\n",
        "    coverage_gt_0_5 = (filtered_df['original'] >= filtered_df['lower_bound']) & (filtered_df['original'] <= filtered_df['upper_bound'])\n",
        "    coverage_gt_0_5_pct = coverage_gt_0_5.mean() * 100\n",
        "\n",
        "    return coverage_pct, average_range_width, coverage_gt_0_5_pct\n",
        "\n",
        "def evaluate_point_predictions(data):\n",
        "    df = pd.DataFrame(data)  # Ensure DataFrame format\n",
        "\n",
        "    mse_mean = np.mean((df['original'] - df['point_pred_mean']) ** 2)\n",
        "    rmse_mean = np.sqrt(mse_mean)\n",
        "\n",
        "    # MSE and RMSE for median predictions\n",
        "    mse_median = np.mean((df['original'] - df['point_pred_median']) ** 2)\n",
        "    rmse_median = np.sqrt(mse_median)\n",
        "\n",
        "    mse_avg = np.mean((df['original'] - df['lower_upper_avg']) ** 2)\n",
        "    rmse_avg = np.sqrt(mse_avg)\n",
        "\n",
        "    # MAE for mean predictions\n",
        "    mae_mean = np.mean(np.abs(df['original'] - df['point_pred_mean']))\n",
        "\n",
        "    # MAE for median predictions\n",
        "    mae_median = np.mean(np.abs(df['original'] - df['point_pred_median']))\n",
        "\n",
        "    mae_avg = np.mean(np.abs(df['original'] - df['lower_upper_avg']))\n",
        "\n",
        "    return mse_mean, rmse_mean, mae_mean,  mse_median, rmse_median , mae_median, mse_avg, rmse_avg, mae_avg\n",
        "\n",
        "def evaluate_range_predictions_new(data):\n",
        "    df = pd.DataFrame(data)  # Ensure DataFrame format\n",
        "\n",
        "    # Coverage: Proportion of original values within bounds\n",
        "    coverage = (df['original'] >= df['lower_bound']) & (df['original'] <= df['upper_bound'])\n",
        "    coverage_pct = coverage.mean() * 100\n",
        "\n",
        "    # Average range width for different buckets\n",
        "    bins = [0, 0.13, 0.25, 0.51, 0.95, 1]  # Define your buckets\n",
        "    labels = ['0-0.13', '0.13-0.25', '0.25-0.51', '0.51-0.95', '0.95-1']\n",
        "    df['bucket'] = pd.cut(df['original'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "    average_range_widths = {}\n",
        "    for bucket in df['bucket'].unique():\n",
        "        bucket_df = df[df['bucket'] == bucket]\n",
        "        average_range_width = (bucket_df['upper_bound'] - bucket_df['lower_bound']).mean()\n",
        "        average_range_widths[bucket] = average_range_width\n",
        "\n",
        "    filtered_df = df[df['original'] > 0.5]  # Filter for points > 0.5\n",
        "    coverage_gt_0_5 = (filtered_df['original'] >= filtered_df['lower_bound']) & (filtered_df['original'] <= filtered_df['upper_bound'])\n",
        "    coverage_gt_0_5_pct = coverage_gt_0_5.mean() * 100\n",
        "\n",
        "    return coverage_pct, average_range_widths, coverage_gt_0_5_pct\n",
        "\n",
        "def evaluate_range_predictions_new_c(data):\n",
        "    df = pd.DataFrame(data)  # Ensure DataFrame format\n",
        "\n",
        "    # Coverage: Proportion of original values within bounds\n",
        "    coverage = (df['original'] >= df['lower_bound']) & (df['original'] <= df['upper_bound'])\n",
        "    coverage_pct = coverage.mean() * 100\n",
        "\n",
        "    # Average range width and coverage for different buckets\n",
        "    bins = [0, 0.13, 0.25, 0.51, 0.95, 1]  # Define your buckets\n",
        "    labels = ['0-0.13', '0.13-0.25', '0.25-0.51', '0.51-0.95', '0.95-1']\n",
        "    df['bucket'] = pd.cut(df['original'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "    average_range_widths = {}\n",
        "    coverage_percentages = {}\n",
        "    for bucket in df['bucket'].unique():\n",
        "        bucket_df = df[df['bucket'] == bucket]\n",
        "        average_range_width = (bucket_df['upper_bound'] - bucket_df['lower_bound']).mean()\n",
        "        average_range_widths[bucket] = average_range_width\n",
        "\n",
        "        # Calculate coverage for the bucket\n",
        "        bucket_coverage = ((bucket_df['original'] >= bucket_df['lower_bound']) &\n",
        "                           (bucket_df['original'] <= bucket_df['upper_bound'])).mean() * 100\n",
        "        coverage_percentages[bucket] = bucket_coverage\n",
        "\n",
        "    filtered_df = df[df['original'] > 0.5]  # Filter for points > 0.5\n",
        "    coverage_gt_0_5 = (filtered_df['original'] >= filtered_df['lower_bound']) & (filtered_df['original'] <= filtered_df['upper_bound'])\n",
        "    coverage_gt_0_5_pct = coverage_gt_0_5.mean() * 100\n",
        "\n",
        "    return coverage_pct, average_range_widths, coverage_gt_0_5_pct, coverage_percentages\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B2SCpFASrQNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results_1(data):\n",
        "  test_coverage_pct, test_average_range_width, test_coverage_pct_gt_0_5_pct = evaluate_range_predictions(data)\n",
        "  print(f\"test Coverage: {test_coverage_pct:.4f}%\")\n",
        "  print(f\"test Average Range Width: {test_average_range_width:.4f}\")\n",
        "  print(f\"test Coverage > 0.5: {test_coverage_pct_gt_0_5_pct:.4f}%\")\n",
        "\n",
        "def print_results_2(data):\n",
        "  t_c , t_b_w , t_c_gt_0_5_pct = evaluate_range_predictions_new(data)\n",
        "  print(f\"coverage: {t_c}\")\n",
        "  print(f\"average range width: {t_b_w}\")\n",
        "  print(f\"coverage > 0.5: {t_c_gt_0_5_pct}\")\n",
        "\n",
        "def print_results_3(data):\n",
        "  t1,t2,t3,t4 = evaluate_range_predictions_new_c(data)\n",
        "  print(\"Coverage percentages per bucket\",t4)\n",
        "\n",
        "def print_point_results(data,datatype):\n",
        "  test_mse_mean, test_rmse_mean, test_mae_mean, test_mse_median, test_rmse_median, test_mae_median, test_avg_mse, test_avg_rmse, test_avg_mae = evaluate_point_predictions(data)\n",
        "  print(f\"{datatype} mse mean: {test_mse_mean:.4f}\")\n",
        "  print(f\"{datatype} rmse mean: {test_rmse_mean:.4f}\")\n",
        "  print(f\"{datatype} mae mean: {test_mae_mean:.4f}\")\n",
        "  print(f\"{datatype} mse median: {test_mse_median:.4f}\")\n",
        "  print(f\"{datatype} rmse median: {test_rmse_median:.4f}\")\n",
        "  print(f\"{datatype} mae median: {test_mae_median:.4f}\")\n",
        "  print(f\"{datatype} avg mse: {test_avg_mse:.4f}\")\n",
        "  print(f\"{datatype} avg rmse: {test_avg_rmse:.4f}\")\n",
        "  print(f\"{datatype} avg mae: {test_avg_mae:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xZlPZXa2rS91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = calculate_intervals(hdi,y_val,val_point_predictions_mean,val_point_predictions_median)"
      ],
      "metadata": {
        "id": "-sbm9IIird5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_1(val_data)"
      ],
      "metadata": {
        "id": "8gztlzYarefZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_2(val_data)"
      ],
      "metadata": {
        "id": "koqJ1cVFrg70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_3(val_data)"
      ],
      "metadata": {
        "id": "2JcUvyqFri4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_point_results(val_data,\"val\")"
      ],
      "metadata": {
        "id": "GIZvpAaCrk0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(data,title):\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Select a random subset\n",
        "    df_plot = df.sample(n=50, random_state=41)\n",
        "\n",
        "    # Assign sequential indexes (starting from 0)\n",
        "    df_plot['index'] = range(len(df_plot))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "\n",
        "    plt.scatter(df_plot['index'], df_plot['original'], marker='o', s=100, label='Original Values')\n",
        "    # Scatter plot for median predictions\n",
        "    plt.scatter(df_plot['index'], df_plot['point_pred_median'], marker='o', s=100, label='Median Predictions', color='red')\n",
        "    plt.plot(df_plot['index'], df_plot['lower_bound'], color='lightgray', label='Lower Bound')\n",
        "    plt.plot(df_plot['index'], df_plot['upper_bound'], color='lightgray', label='Upper Bound')\n",
        "\n",
        "    # Connect upper and lower bounds with lines for clarity\n",
        "    for i in range(len(df_plot)):\n",
        "      plt.plot([df_plot['index'].iloc[i], df_plot['index'].iloc[i]],\n",
        "              [df_plot['lower_bound'].iloc[i], df_plot['upper_bound'].iloc[i]],\n",
        "              color='gray', linewidth=1)\n",
        "\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fog Index Values')\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.legend()  # Add legend for clarity\n",
        "    plt.tight_layout()  # Adjust spacing for labels and title\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sVC8d6KpU1I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(val_data,\"Validation Data Predictions - Beta Regression (1 day)\")"
      ],
      "metadata": {
        "id": "xjjKcSlmU6k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update with the appropriate testing data path\n",
        "df_winter_test = pd.read_csv('',index_col=0, parse_dates=True)\n",
        "\n",
        "df_winter_test = df_winter_test.dropna()\n",
        "\n",
        "X_test = df_winter_test.drop([\"fog_index_6h\",\"fog_index_1d\"], axis=1)  # Features\n",
        "y_test = df_winter_test[\"fog_index_1d\"]  # Target variable\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
      ],
      "metadata": {
        "id": "JjysNlmqU-tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with beta_regression_model:\n",
        "    pm.set_data({\n",
        "        'avg_air_temp': X_test_scaled_df['avg_air_temp'],\n",
        "        'avg_dew_point': X_test_scaled_df['avg_dew_point'],\n",
        "        'avg_relative_humidity': X_test_scaled_df['avg_relative_humidity'],\n",
        "        'avg_pressure': X_test_scaled_df['avg_pressure'],\n",
        "        'avg_visibility': X_test_scaled_df['avg_visibility'],\n",
        "        'energy_loss': X_test_scaled_df['energy_loss'],\n",
        "        'fog_duration': X_test_scaled_df['fog_duration'],\n",
        "        'fog_month': X_test_scaled_df['fog_month'],\n",
        "        'fog_index': y_test\n",
        "    })\n",
        "\n",
        "\n",
        "    idata_test = pm.sample_posterior_predictive(\n",
        "                trace,\n",
        "                var_names=[\"mu\",\"fog_index_var\"],\n",
        "                return_inferencedata=True,\n",
        "                predictions=True,\n",
        "\n",
        "                random_seed=42  # Or any seed\n",
        "     )"
      ],
      "metadata": {
        "id": "iX_g4JMqVB_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idata_test"
      ],
      "metadata": {
        "id": "0IZxck_hVEvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(idata_test)"
      ],
      "metadata": {
        "id": "lSTvOg2UVG7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(idata_test.predictions)"
      ],
      "metadata": {
        "id": "kQcsosFXVJEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_for_test_point = idata_test.predictions['fog_index_var']  # Sample some draws\n",
        "\n",
        "# HDI Calculation\n",
        "hdi = pm.hdi(predictions_for_test_point, hdi_prob=0.95)\n",
        "print(hdi)\n",
        "\n",
        "hdi_90 = pm.hdi(predictions_for_test_point, hdi_prob=0.9)\n",
        "hdi_99 = pm.hdi(predictions_for_test_point, hdi_prob=0.99)"
      ],
      "metadata": {
        "id": "Rpzxy_o4VUE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_point_predictions_mean = idata_test.predictions['fog_index_var'].mean(dim=['chain', 'draw']).values\n",
        "test_point_predictions_median = idata_test.predictions['fog_index_var'].median(dim=['chain', 'draw']).values"
      ],
      "metadata": {
        "id": "eqBpx0TASf1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_hdi_95_cal_data = calculate_intervals(hdi,y_test,test_point_predictions_mean,test_point_predictions_median)\n",
        "test_hdi_90_cal_data = calculate_intervals(hdi_90,y_test,test_point_predictions_mean,test_point_predictions_median)\n",
        "test_hdi_99_cal_data = calculate_intervals(hdi_99,y_test,test_point_predictions_mean,test_point_predictions_median)"
      ],
      "metadata": {
        "id": "sKgfSuY9r7c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_1(test_hdi_95_cal_data)"
      ],
      "metadata": {
        "id": "mAZAmgu6r-MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_1(test_hdi_90_cal_data)"
      ],
      "metadata": {
        "id": "kUj81spbr-yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_1(test_hdi_99_cal_data)"
      ],
      "metadata": {
        "id": "13bx9YcvsAnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_2(test_hdi_95_cal_data)"
      ],
      "metadata": {
        "id": "VTJhw4WBsC5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_2(test_hdi_90_cal_data)"
      ],
      "metadata": {
        "id": "cDZcUeT5sE9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_2(test_hdi_99_cal_data)"
      ],
      "metadata": {
        "id": "nQPIG4KesGpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_3(test_hdi_95_cal_data)"
      ],
      "metadata": {
        "id": "q-vvoNEIsIy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_3(test_hdi_90_cal_data)"
      ],
      "metadata": {
        "id": "oqhzpcnhsTkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_3(test_hdi_99_cal_data)"
      ],
      "metadata": {
        "id": "sGd4yjPLsUPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_point_results(test_hdi_95_cal_data,\"test\")"
      ],
      "metadata": {
        "id": "jyTzFKOosWOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(test_hdi_90_cal_data,\"Test Data Predictions -Beta Regression (1 day)\")"
      ],
      "metadata": {
        "id": "bPz5bLeJVekj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}