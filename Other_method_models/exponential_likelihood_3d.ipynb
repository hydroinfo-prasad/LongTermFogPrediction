{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_0xvPI6LnVu"
      },
      "outputs": [],
      "source": [
        "!pip install pymc==5.10.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZckYibiLztd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SWept5eIbAdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_GTvrNRL3Jc"
      },
      "outputs": [],
      "source": [
        "# update with the appropriate training data path\n",
        "df_winter = pd.read_csv('',index_col=0, parse_dates=True)\n",
        "\n",
        "df_winter.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD01jvOkMDb9"
      },
      "outputs": [],
      "source": [
        "df_winter.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XltnmpLyMEnE"
      },
      "outputs": [],
      "source": [
        "df_winter = df_winter.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0gH3FKTMIGk"
      },
      "outputs": [],
      "source": [
        "df_winter.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOOAke9qMLtE"
      },
      "outputs": [],
      "source": [
        "df_winter['fog_index_3d'].eq(0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhuDBYE0MOqM"
      },
      "outputs": [],
      "source": [
        "df_winter.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byeDtIsrMRrc"
      },
      "outputs": [],
      "source": [
        "df_filtered = df_winter[df_winter['fog_index_3d'] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLeGf0xfMWPs"
      },
      "outputs": [],
      "source": [
        "df_winter = df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtgjbBPpMbis"
      },
      "outputs": [],
      "source": [
        "df_winter.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QHB6ntPMfmz"
      },
      "outputs": [],
      "source": [
        "df_winter['fog_index_3d'].eq(0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_winter = df_winter[df_winter['energy_loss'] <= 1500000]"
      ],
      "metadata": {
        "id": "TGktKncJY6Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51U0p9tnMilL"
      },
      "outputs": [],
      "source": [
        "# Get descriptive statistics\n",
        "descriptive_stats = df_winter[\"fog_index_6h\"].describe()\n",
        "\n",
        "# Print the results\n",
        "print(descriptive_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1_xD-IiMlbs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGl6zAbgMows"
      },
      "outputs": [],
      "source": [
        "X = df_winter.drop([\"fog_index_6h\",\"fog_index_1d\",\"fog_index_3d\"], axis=1)  # Features\n",
        "y = df_winter[\"fog_index_3d\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoNhrel1MvIb"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X)  # Fit scaler on training data\n",
        "\n",
        "X_train_scaled = scaler.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLrpA2mMMyHT"
      },
      "outputs": [],
      "source": [
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IyCpRqwM4X7"
      },
      "outputs": [],
      "source": [
        "X_train_scaled_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y"
      ],
      "metadata": {
        "id": "EmjpVBmlThAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR52eNBiM6-8"
      },
      "outputs": [],
      "source": [
        "import pymc as pm\n",
        "\n",
        "with pm.Model() as beta_regression_model:\n",
        "    # Priors\n",
        "    beta_0 = pm.Normal('beta_0', mu=0, sigma=100)\n",
        "    beta_1 = pm.Normal('beta_1', mu=0, sigma=100)\n",
        "    beta_2 = pm.Normal('beta_2', mu=0, sigma=100)\n",
        "    beta_3 = pm.Normal('beta_3', mu=0, sigma=100)\n",
        "    beta_4 = pm.Normal('beta_4', mu=0, sigma=100)\n",
        "    beta_5 = pm.Normal('beta_5', mu=0, sigma=100)\n",
        "    beta_6 = pm.Normal('beta_6', mu=0, sigma=100)\n",
        "    beta_7 = pm.Normal('beta_7', mu=0, sigma=100)\n",
        "    beta_8 = pm.Normal('beta_8', mu=0, sigma=100)\n",
        "\n",
        "    avg_air_temp = pm.MutableData('avg_air_temp', X_train_scaled_df['avg_air_temp'])\n",
        "    avg_dew_point = pm.MutableData('avg_dew_point', X_train_scaled_df['avg_dew_point'])\n",
        "    avg_relative_humidity = pm.MutableData('avg_relative_humidity', X_train_scaled_df['avg_relative_humidity'])\n",
        "    avg_pressure = pm.MutableData('avg_pressure', X_train_scaled_df['avg_pressure'])\n",
        "    avg_visibility = pm.MutableData('avg_visibility', X_train_scaled_df['avg_visibility'])\n",
        "    energy_loss = pm.MutableData('energy_loss', X_train_scaled_df['energy_loss'])\n",
        "    fog_duration = pm.MutableData('fog_duration', X_train_scaled_df['fog_duration'])\n",
        "    fog_month = pm.MutableData('fog_month', X_train_scaled_df['fog_month'])\n",
        "\n",
        "    fog_index_3d = pm.MutableData('fog_index', y_train)\n",
        "\n",
        "\n",
        "    # scale = pm.HalfNormal(\"scale\", sigma=10)\n",
        "\n",
        "    # Mean of the beta distribution\n",
        "    mu = pm.Deterministic('mu', pm.math.invlogit(beta_0 + beta_1 * avg_air_temp +\n",
        "                          beta_2 * avg_dew_point +\n",
        "                          beta_3 * avg_relative_humidity +\n",
        "                          beta_4 * avg_pressure +\n",
        "                          beta_5 * avg_visibility +\n",
        "                          beta_6 * energy_loss +\n",
        "                          beta_7 * fog_duration +\n",
        "                          beta_8 * fog_month))\n",
        "\n",
        "    exponential_dist = pm.Exponential.dist(lam=1/mu)  # Exponential likelihood\n",
        "    pm.Truncated(\"fog_index_var\", exponential_dist, lower=0, upper=1, observed=fog_index_3d)\n",
        "\n",
        "    # Inference\n",
        "    trace = pm.sample(draws=2000, tune=2000, chains=2,return_inferencedata=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV9e-hUBPE0q"
      },
      "outputs": [],
      "source": [
        "import arviz as az\n",
        "az.plot_trace(trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Um6xktRQWwa"
      },
      "outputs": [],
      "source": [
        "az.summary(trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHGoZDFsQoV6"
      },
      "outputs": [],
      "source": [
        "import arviz as az\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oubHWPF5QuAZ"
      },
      "outputs": [],
      "source": [
        "with beta_regression_model:  # Reuse the model context\n",
        "    ppd = pm.sample_posterior_predictive(\n",
        "        trace,\n",
        "        var_names=['mu','fog_index_var'],  # Name of output variable\n",
        "        random_seed=42  # Optional, set a seed for reproducibility\n",
        "    )\n",
        "    az.plot_ppc(ppd)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kukSSpJsRD4S"
      },
      "outputs": [],
      "source": [
        "trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0_oB8heRIFS"
      },
      "outputs": [],
      "source": [
        "ppd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update with the appropriate validation data path\n",
        "df_winter_val = pd.read_csv('',index_col=0, parse_dates=True)\n",
        "\n",
        "df_winter_val = df_winter_val.dropna()\n",
        "\n",
        "X_val = df_winter_val.drop([\"fog_index_6h\",\"fog_index_1d\",\"fog_index_3d\"], axis=1)  # Features\n",
        "y_val = df_winter_val[\"fog_index_3d\"]  # Target variable\n",
        "\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)"
      ],
      "metadata": {
        "id": "gCRwp1nvY1Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULN-FzPQRLTx"
      },
      "outputs": [],
      "source": [
        "with beta_regression_model:\n",
        "    pm.set_data({\n",
        "        'avg_air_temp': X_val_scaled_df['avg_air_temp'],\n",
        "        'avg_dew_point': X_val_scaled_df['avg_dew_point'],\n",
        "        'avg_relative_humidity': X_val_scaled_df['avg_relative_humidity'],\n",
        "        'avg_pressure': X_val_scaled_df['avg_pressure'],\n",
        "        'avg_visibility': X_val_scaled_df['avg_visibility'],\n",
        "        'energy_loss': X_val_scaled_df['energy_loss'],\n",
        "        'fog_duration': X_val_scaled_df['fog_duration'],\n",
        "        'fog_month': X_val_scaled_df['fog_month'],\n",
        "        'fog_index': y_val\n",
        "    })\n",
        "\n",
        "    idata_val = pm.sample_posterior_predictive(\n",
        "                trace,\n",
        "                var_names=[\"mu\",\"fog_index_var\"],\n",
        "                return_inferencedata=True,\n",
        "                predictions=True,\n",
        "                extend_inferencedata=True,\n",
        "                random_seed=42  # Or any seed\n",
        "     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghoRfFyRRPTL"
      },
      "outputs": [],
      "source": [
        "idata_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRvroFQXRSWz"
      },
      "outputs": [],
      "source": [
        "print(idata_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJmfBAluRVDz"
      },
      "outputs": [],
      "source": [
        "print(idata_val.predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5kSSnhPRYB2"
      },
      "outputs": [],
      "source": [
        "predictions_for_val_data = idata_val.predictions['fog_index_var']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pHkLoGh-SXm"
      },
      "outputs": [],
      "source": [
        "predictions_for_val_point = idata_val.predictions['fog_index_var']  # Sample some draws\n",
        "\n",
        "# HDI Calculation\n",
        "hdi = pm.hdi(predictions_for_val_point, hdi_prob=0.95)\n",
        "print(hdi)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_point_predictions_mean = idata_val.predictions['fog_index_var'].mean(dim=['chain', 'draw']).values\n",
        "val_point_predictions_median = idata_val.predictions['fog_index_var'].median(dim=['chain', 'draw']).values"
      ],
      "metadata": {
        "id": "AKLyHynd5EOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_intervals(hdi,y_test,pp_mean,pp_median):\n",
        "  test_lower_bounds = []\n",
        "  test_upper_bounds = []\n",
        "  test_original_values = []\n",
        "  test_point_pred_mean = []\n",
        "  test_point_pred_median =[]\n",
        "  test_lower_upper_avg = []\n",
        "  test_dates = []\n",
        "\n",
        "  for idx in range(len(hdi['fog_index_var_dim_2'])):\n",
        "    lower_bound = hdi.sel(fog_index_var_dim_2=idx, hdi='lower')['fog_index_var'].values\n",
        "    upper_bound = hdi.sel(fog_index_var_dim_2=idx, hdi='higher')['fog_index_var'].values\n",
        "    avg = (upper_bound - lower_bound)/2\n",
        "\n",
        "    # print(lower_bound['fog_index_var'].values)\n",
        "\n",
        "    point_pred_mean = pp_mean[idx]\n",
        "    point_pred_median = pp_median[idx]\n",
        "    # lb = lower_bound['fog_index_var'].values\n",
        "    # ub = upper_bound['fog_index_var'].values\n",
        "    # break\n",
        "    original = y_test[idx]\n",
        "    date = y_test.index[idx]\n",
        "    # print(f\"Test Point: {idx}, Range: ({lower_bound:.2f}, {upper_bound:.2f}) ,point mean: {point_pred_mean:.2f},point median: {point_pred_median:.2f}, Original: {original:.2f}, Date: {date}\")\n",
        "    test_lower_bounds.append(lower_bound)\n",
        "    test_upper_bounds.append(upper_bound)\n",
        "    test_point_pred_mean.append(point_pred_mean)\n",
        "    test_point_pred_median.append(point_pred_median)\n",
        "    test_lower_upper_avg.append(avg)\n",
        "    test_original_values.append(original)\n",
        "    test_dates.append(date)\n",
        "\n",
        "  data = {\n",
        "    'lower_bound': test_lower_bounds,\n",
        "    'upper_bound': test_upper_bounds,\n",
        "    'original': test_original_values,\n",
        "    'point_pred_mean': test_point_pred_mean,\n",
        "    'point_pred_median': test_point_pred_median,\n",
        "    'lower_upper_avg': test_lower_upper_avg,\n",
        "    'date': test_dates\n",
        "  }\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "b5HF0b5a5B_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def evaluate_range_predictions(data):\n",
        "    df = pd.DataFrame(data)  # Ensure DataFrame format\n",
        "\n",
        "    # Coverage: Proportion of original values within bounds\n",
        "    coverage = (df['original'] >= df['lower_bound']) & (df['original'] <= df['upper_bound'])\n",
        "    coverage_pct = coverage.mean() * 100\n",
        "\n",
        "    # Average range width\n",
        "    average_range_width = df['upper_bound'] - df['lower_bound']\n",
        "    average_range_width = average_range_width.mean()\n",
        "\n",
        "    filtered_df = df[df['original'] > 0.5]  # Filter for points > 0.5\n",
        "    coverage_gt_0_5 = (filtered_df['original'] >= filtered_df['lower_bound']) & (filtered_df['original'] <= filtered_df['upper_bound'])\n",
        "    coverage_gt_0_5_pct = coverage_gt_0_5.mean() * 100\n",
        "\n",
        "    return coverage_pct, average_range_width, coverage_gt_0_5_pct\n",
        "\n",
        "def evaluate_point_predictions(data):\n",
        "    df = pd.DataFrame(data)  # Ensure DataFrame format\n",
        "\n",
        "    mse_mean = np.mean((df['original'] - df['point_pred_mean']) ** 2)\n",
        "    rmse_mean = np.sqrt(mse_mean)\n",
        "\n",
        "    # MSE and RMSE for median predictions\n",
        "    mse_median = np.mean((df['original'] - df['point_pred_median']) ** 2)\n",
        "    rmse_median = np.sqrt(mse_median)\n",
        "\n",
        "    mse_avg = np.mean((df['original'] - df['lower_upper_avg']) ** 2)\n",
        "    rmse_avg = np.sqrt(mse_avg)\n",
        "\n",
        "    # MAE for mean predictions\n",
        "    mae_mean = np.mean(np.abs(df['original'] - df['point_pred_mean']))\n",
        "\n",
        "    # MAE for median predictions\n",
        "    mae_median = np.mean(np.abs(df['original'] - df['point_pred_median']))\n",
        "\n",
        "    mae_avg = np.mean(np.abs(df['original'] - df['lower_upper_avg']))\n",
        "\n",
        "    return mse_mean, rmse_mean, mae_mean,  mse_median, rmse_median , mae_median, mse_avg, rmse_avg, mae_avg\n",
        "\n",
        "def evaluate_range_predictions_new(data):\n",
        "    df = pd.DataFrame(data)  # Ensure DataFrame format\n",
        "\n",
        "    # Coverage: Proportion of original values within bounds\n",
        "    coverage = (df['original'] >= df['lower_bound']) & (df['original'] <= df['upper_bound'])\n",
        "    coverage_pct = coverage.mean() * 100\n",
        "\n",
        "    # Average range width for different buckets\n",
        "    bins = [0, 0.13, 0.25, 0.51, 0.95, 1]  # Define your buckets\n",
        "    labels = ['0-0.13', '0.13-0.25', '0.25-0.51', '0.51-0.95', '0.95-1']\n",
        "    df['bucket'] = pd.cut(df['original'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "    average_range_widths = {}\n",
        "    for bucket in df['bucket'].unique():\n",
        "        bucket_df = df[df['bucket'] == bucket]\n",
        "        average_range_width = (bucket_df['upper_bound'] - bucket_df['lower_bound']).mean()\n",
        "        average_range_widths[bucket] = average_range_width\n",
        "\n",
        "    filtered_df = df[df['original'] > 0.5]  # Filter for points > 0.5\n",
        "    coverage_gt_0_5 = (filtered_df['original'] >= filtered_df['lower_bound']) & (filtered_df['original'] <= filtered_df['upper_bound'])\n",
        "    coverage_gt_0_5_pct = coverage_gt_0_5.mean() * 100\n",
        "\n",
        "    return coverage_pct, average_range_widths, coverage_gt_0_5_pct\n",
        "\n",
        "def evaluate_range_predictions_new_c(data):\n",
        "    df = pd.DataFrame(data)  # Ensure DataFrame format\n",
        "\n",
        "    # Coverage: Proportion of original values within bounds\n",
        "    coverage = (df['original'] >= df['lower_bound']) & (df['original'] <= df['upper_bound'])\n",
        "    coverage_pct = coverage.mean() * 100\n",
        "\n",
        "    # Average range width and coverage for different buckets\n",
        "    bins = [0, 0.13, 0.25, 0.51, 0.95, 1]  # Define your buckets\n",
        "    labels = ['0-0.13', '0.13-0.25', '0.25-0.51', '0.51-0.95', '0.95-1']\n",
        "    df['bucket'] = pd.cut(df['original'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "    average_range_widths = {}\n",
        "    coverage_percentages = {}\n",
        "    for bucket in df['bucket'].unique():\n",
        "        bucket_df = df[df['bucket'] == bucket]\n",
        "        average_range_width = (bucket_df['upper_bound'] - bucket_df['lower_bound']).mean()\n",
        "        average_range_widths[bucket] = average_range_width\n",
        "\n",
        "        # Calculate coverage for the bucket\n",
        "        bucket_coverage = ((bucket_df['original'] >= bucket_df['lower_bound']) &\n",
        "                           (bucket_df['original'] <= bucket_df['upper_bound'])).mean() * 100\n",
        "        coverage_percentages[bucket] = bucket_coverage\n",
        "\n",
        "    filtered_df = df[df['original'] > 0.5]  # Filter for points > 0.5\n",
        "    coverage_gt_0_5 = (filtered_df['original'] >= filtered_df['lower_bound']) & (filtered_df['original'] <= filtered_df['upper_bound'])\n",
        "    coverage_gt_0_5_pct = coverage_gt_0_5.mean() * 100\n",
        "\n",
        "    return coverage_pct, average_range_widths, coverage_gt_0_5_pct, coverage_percentages"
      ],
      "metadata": {
        "id": "WX5lwHs75JNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results_1(data):\n",
        "  test_coverage_pct, test_average_range_width, test_coverage_pct_gt_0_5_pct = evaluate_range_predictions(data)\n",
        "  print(f\"test Coverage: {test_coverage_pct:.4f}%\")\n",
        "  print(f\"test Average Range Width: {test_average_range_width:.4f}\")\n",
        "  print(f\"test Coverage > 0.5: {test_coverage_pct_gt_0_5_pct:.4f}%\")\n",
        "\n",
        "def print_results_2(data):\n",
        "  t_c , t_b_w , t_c_gt_0_5_pct = evaluate_range_predictions_new(data)\n",
        "  print(f\"coverage: {t_c}\")\n",
        "  print(f\"average range width: {t_b_w}\")\n",
        "  print(f\"coverage > 0.5: {t_c_gt_0_5_pct}\")\n",
        "\n",
        "def print_results_3(data):\n",
        "  t1,t2,t3,t4 = evaluate_range_predictions_new_c(data)\n",
        "  print(\"Coverage percentages per bucket\",t4)\n",
        "\n",
        "def print_point_results(data,datatype):\n",
        "  test_mse_mean, test_rmse_mean, test_mae_mean, test_mse_median, test_rmse_median, test_mae_median, test_avg_mse, test_avg_rmse, test_avg_mae = evaluate_point_predictions(data)\n",
        "  print(f\"{datatype} mse mean: {test_mse_mean:.4f}\")\n",
        "  print(f\"{datatype} rmse mean: {test_rmse_mean:.4f}\")\n",
        "  print(f\"{datatype} mae mean: {test_mae_mean:.4f}\")\n",
        "  print(f\"{datatype} mse median: {test_mse_median:.4f}\")\n",
        "  print(f\"{datatype} rmse median: {test_rmse_median:.4f}\")\n",
        "  print(f\"{datatype} mae median: {test_mae_median:.4f}\")\n",
        "  print(f\"{datatype} avg mse: {test_avg_mse:.4f}\")\n",
        "  print(f\"{datatype} avg rmse: {test_avg_rmse:.4f}\")\n",
        "  print(f\"{datatype} avg mae: {test_avg_mae:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CfLtAzNV5OAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = calculate_intervals(hdi,y_val,val_point_predictions_mean,val_point_predictions_median)"
      ],
      "metadata": {
        "id": "XesIA4_Q5Wj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_1(val_data)"
      ],
      "metadata": {
        "id": "XvTrtYUE5bQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_2(val_data)"
      ],
      "metadata": {
        "id": "SaXrf8rd5dRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_3(val_data)"
      ],
      "metadata": {
        "id": "oUxwG0q75gIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_point_results(val_data,\"val\")"
      ],
      "metadata": {
        "id": "bv9ctQH25iwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(data,title):\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Select a random subset\n",
        "    df_plot = df.sample(n=50, random_state=42)\n",
        "\n",
        "    # Assign sequential indexes (starting from 0)\n",
        "    df_plot['index'] = range(len(df_plot))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "\n",
        "    plt.scatter(df_plot['index'], df_plot['original'], marker='o', s=100, label='Original Values')\n",
        "    # Scatter plot for median predictions\n",
        "    plt.scatter(df_plot['index'], df_plot['point_pred_median'], marker='o', s=100, label='Median Predictions', color='red')\n",
        "    plt.plot(df_plot['index'], df_plot['lower_bound'], color='lightgray', label='Lower Bound')\n",
        "    plt.plot(df_plot['index'], df_plot['upper_bound'], color='lightgray', label='Upper Bound')\n",
        "\n",
        "    # Connect upper and lower bounds with lines for clarity\n",
        "    for i in range(len(df_plot)):\n",
        "      plt.plot([df_plot['index'].iloc[i], df_plot['index'].iloc[i]],\n",
        "              [df_plot['lower_bound'].iloc[i], df_plot['upper_bound'].iloc[i]],\n",
        "              color='gray', linewidth=1)\n",
        "\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Fog Index Values')\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.legend()  # Add legend for clarity\n",
        "    plt.tight_layout()  # Adjust spacing for labels and title\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lMs0pEyCjKTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(val_data,\"Validation Data Predictions - Exponential Likelihood ( 3 days)\")"
      ],
      "metadata": {
        "id": "rs3AZOq0mExK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update with the appropriate testing data path\n",
        "df_winter_test = pd.read_csv('',index_col=0, parse_dates=True)\n",
        "\n",
        "df_winter_test = df_winter_test.dropna()\n",
        "\n",
        "X_test = df_winter_test.drop([\"fog_index_6h\",\"fog_index_1d\",\"fog_index_3d\"], axis=1)  # Features\n",
        "y_test = df_winter_test[\"fog_index_3d\"]  # Target variable\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
      ],
      "metadata": {
        "id": "C5gWfx89pD54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with beta_regression_model:\n",
        "    pm.set_data({\n",
        "        'avg_air_temp': X_test_scaled_df['avg_air_temp'],\n",
        "        'avg_dew_point': X_test_scaled_df['avg_dew_point'],\n",
        "        'avg_relative_humidity': X_test_scaled_df['avg_relative_humidity'],\n",
        "        'avg_pressure': X_test_scaled_df['avg_pressure'],\n",
        "        'avg_visibility': X_test_scaled_df['avg_visibility'],\n",
        "        'energy_loss': X_test_scaled_df['energy_loss'],\n",
        "        'fog_duration': X_test_scaled_df['fog_duration'],\n",
        "        'fog_month': X_test_scaled_df['fog_month'],\n",
        "        'fog_index': y_test\n",
        "    })\n",
        "\n",
        "    idata_test = pm.sample_posterior_predictive(\n",
        "                trace,\n",
        "                var_names=[\"mu\",\"fog_index_var\"],\n",
        "                return_inferencedata=True,\n",
        "                predictions=True,\n",
        "\n",
        "                random_seed=42  # Or any seed\n",
        "     )"
      ],
      "metadata": {
        "id": "XFKo5I0CpHro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idata_test"
      ],
      "metadata": {
        "id": "1szPOiY_pIk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(idata_test)"
      ],
      "metadata": {
        "id": "HrnDfS73pK4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(idata_test.predictions)"
      ],
      "metadata": {
        "id": "sdMePwDWpNXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_for_test_point = idata_test.predictions['fog_index_var']  # Sample some draws\n",
        "\n",
        "# HDI Calculation\n",
        "hdi = pm.hdi(predictions_for_test_point, hdi_prob=0.95)\n",
        "print(hdi)\n",
        "\n",
        "hdi_90 = pm.hdi(predictions_for_test_point, hdi_prob=0.9)\n",
        "hdi_99 = pm.hdi(predictions_for_test_point, hdi_prob=0.99)"
      ],
      "metadata": {
        "id": "QmUmLJ8XpQcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_point_predictions_mean = idata_test.predictions['fog_index_var'].mean(dim=['chain', 'draw']).values\n",
        "test_point_predictions_median = idata_test.predictions['fog_index_var'].median(dim=['chain', 'draw']).values"
      ],
      "metadata": {
        "id": "BCj1jclo1ASS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_hdi_95_cal_data = calculate_intervals(hdi,y_test,test_point_predictions_mean,test_point_predictions_median)\n",
        "test_hdi_90_cal_data = calculate_intervals(hdi_90,y_test,test_point_predictions_mean,test_point_predictions_median)\n",
        "test_hdi_99_cal_data = calculate_intervals(hdi_99,y_test,test_point_predictions_mean,test_point_predictions_median)"
      ],
      "metadata": {
        "id": "CRkDQ5jT6u-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_1(test_hdi_95_cal_data)"
      ],
      "metadata": {
        "id": "lfyahx0T6vnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_1(test_hdi_90_cal_data)"
      ],
      "metadata": {
        "id": "afkrCPv46x0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_1(test_hdi_99_cal_data)"
      ],
      "metadata": {
        "id": "p4Jjy8pw60NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_2(test_hdi_95_cal_data)"
      ],
      "metadata": {
        "id": "87JoOqZ362Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_2(test_hdi_90_cal_data)"
      ],
      "metadata": {
        "id": "tsWDjAmd64rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_2(test_hdi_99_cal_data)"
      ],
      "metadata": {
        "id": "FKxEZ3-c67Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_3(test_hdi_95_cal_data)"
      ],
      "metadata": {
        "id": "6TyFtY7t69Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_3(test_hdi_90_cal_data)"
      ],
      "metadata": {
        "id": "pOyN_OuI7Azr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_results_3(test_hdi_99_cal_data)"
      ],
      "metadata": {
        "id": "nyViKuUS7C8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_point_results(test_hdi_95_cal_data,\"test\")"
      ],
      "metadata": {
        "id": "j9GsTr5l7Ffw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(test_hdi_95_cal_data,\"Test Data Predictions - Exponential likelihood (3 days)\")"
      ],
      "metadata": {
        "id": "NICReulTpgVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}